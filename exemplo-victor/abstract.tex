Snapshot testing is a software testing technique in which the output of a component—such as a user interface rendered in tags or a data structure—is captured and saved as a snapshot. This snapshot serves as a reference point and is later compared against future outputs to identify unintended changes, assisting in consistency across application updates. Despite its widespread industrial adoption, there is a scarcity of academic literature exploring its nuances and best practices. This master dissertation aims to help bridge that gap by providing a detailed analysis of the grey literature and conducting an empirical study on the practice of snapshot testing with Jest. Initially, we compiled and analyzed 50 documents from the grey literature to gain a clearer understanding of the current status of snapshot testing within the software development community. This analysis clarified the benefits and drawbacks of snapshot testing, identified best practices, highlighted prevalent architectural components that adopt this approach, and revealed the most commonly used tools for implementing snapshot tests. We found that snapshot tests are popular because they are easy to implement and effectively prevent regressions, especially in frontend and mobile applications, predominantly using the Jest testing framework. However, drawbacks such as fragility and lack of context can lead to false positives, making it challenging for developers to interpret test failures. To mitigate these issues, best practices like treating snapshot results as part of the application's codebase and writing small, focused snapshots are recommended. Subsequently, we evaluated snapshot testing practices through an empirical study. We conducted a comprehensive investigation into the use of snapshot testing in 569 open-source projects by analyzing a random sample of 380 tests. We aimed to identify the main characteristics of snapshot tests in terms of the components being tested, the format of the generated snapshot files, and their size in lines of code. We identified two common patterns of snapshot tests and four less common cases, including two test smells. In response, we documented two refactoring operations to eliminate these smells. Additionally, we conducted a quantitative study on the dynamic behavior of snapshot tests by measuring their success rates in continuous integration tools. As the key contribution of this work, we present practical guidelines for implementing snapshot tests, which can help developers use these tests more effectively and frequently.